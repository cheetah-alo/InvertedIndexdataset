{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#E8846F\">Dictionary Inverted Index Python Challange<a name=\"id6\"></a>\n",
    "   \n",
    "   by: Jacky Barraza  \n",
    "\n",
    "This notebook is a key part of the requirement done by Sam to Jon Snow. The main goal of the whole project is to build a friendly solution to search for information in documents from keywords. \n",
    "    \n",
    "The main project for big data is presented in the notebook called **SQL-SparkJackyB**\n",
    "    \n",
    "Jon snow understand the importance to have some book collections in private, therefore he developed a python program that will help to Sam do secret search easy to work for him. Can be activeted the subprograms for doing query in specific words.\n",
    "    \n",
    "The program creates the inverted index of documents by giving the path. \n",
    "    \n",
    "The test was done with files located in the local directory.\n",
    "    \n",
    "The result of this process can be saved in a file in a secret directory directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "os.path\n",
    "from collections import defaultdict \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaning propose - contracting mapping\n",
    "# from https://gist.github.com/nealrs/96342d8231b75cf4bb82\n",
    "\n",
    "contraction_mapping = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking access to the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## files path \n",
    "files_path = \"/Users/jackyb/PycharmProjects/4_Inverted_Index_dataset/data/input/dataset_test\"\n",
    "querie = ['christmas', 'snow', 'fireplace', 'happy', 'navidad', 'wrapped', 'prettiest', 'magic']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " => Number of files:  6   => The files:  ['.DS_Store', '1', '4', '3', '2', '5']\n"
     ]
    }
   ],
   "source": [
    "#checking access to the link\n",
    "\n",
    "list_docs = os.listdir(files_path) # files_path is your directory path\n",
    "number_files = len(list_docs)\n",
    "print (\" => Number of files: \",number_files, \"  => The files: \", list_docs )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettingpath():\n",
    "    '''\n",
    "    Prepare the script to ask the client the path to be anlyzed\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    files_path = input('Write the path where are located the files to scan begining by a /: ')\n",
    "    return files_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettingquery():\n",
    "    '''\n",
    "    Asking the client if want to search any specific words\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    listwordsc = input('Write the list of words to be scanned serarated by a space: ')\n",
    "    querie = list(listwordsc.split(\" \"))\n",
    "    return querie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_docs(path):\n",
    "    '''\n",
    "    \n",
    "    Getting the name of the documents in a list\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    all_docs = [f for f in os.listdir(path) if not f.startswith('.')] \n",
    "    all_docs.sort()\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_txt(texto): \n",
    "    '''\n",
    "    \n",
    "    cleaning up the text to get the tokens\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    str_txt = []\n",
    "    ntxt = []\n",
    "    \n",
    "    ## converting txt to string\n",
    "    str_txt = ''.join(texto)\n",
    "    \n",
    "    #print(\"\\n\")\n",
    "    #print('============ text in the file as string. =====================')\n",
    "    #print(str_txt)\n",
    "\n",
    "    # removing contractions\n",
    "    ntxt = str_txt.lower()\n",
    "    ntxt = ' '.join([contraction_mapping[t] if t in contraction_mapping\\\n",
    "           else t for t in ntxt.split(\" \")])\n",
    "\n",
    "    # removing spaecial character\n",
    "    ntxt = re.sub(r'\\([^)]*\\)', '', ntxt)\n",
    "    ntxt = re.sub('\"','', ntxt)\n",
    "    ntxt = re.sub(r\"'s\\b\",\"\",ntxt)\n",
    "    ntxt = re.sub(\"[^a-zA-Z]\", \" \", ntxt)\n",
    "    \n",
    "    #print(\"\\n\")\n",
    "    #print('============ clean text not special characters. =====================')\n",
    "    #print(ntxt)\n",
    "    \n",
    "    return ntxt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokens_alldocs(texto):\n",
    "    '''\n",
    "    getting the main tokens from all the documents and \n",
    "    getting the main tokens input for the final dictionary\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    text_tokens = []\n",
    "    no_sw_tokens_d = []\n",
    "    clean_tokens_d = []\n",
    "    words_d = []\n",
    "\n",
    "    # ## removing stopwords\n",
    "    text_tokens = word_tokenize(texto)\n",
    "    no_sw_tokens_d = [word for word in text_tokens if not word in stopwords.words(\"english\")]\n",
    "    \n",
    "    #print(\"\\n\")\n",
    "    #print('============ no_sw_tokens_d =====================')\n",
    "    #print(no_sw_tokens_d)\n",
    "    \n",
    "    # Identify unique terms in the corpus\n",
    "    for t in no_sw_tokens_d:    \n",
    "        if t not in clean_tokens_d:\n",
    "            clean_tokens_d.append(t)\n",
    "            \n",
    "    #print(\"\\n\")\n",
    "    #print('============ tokens. =====================')\n",
    "    #print(clean_tokens_d)\n",
    "\n",
    "        \n",
    "    for t in clean_tokens_d:\n",
    "        if t not in words_d:\n",
    "            words_d.append(t)\n",
    "    words_d.sort()\n",
    "    #print(\"\\n\")\n",
    "    #print('============ main clean tokens. =====================')\n",
    "    #print(words_d)\n",
    "    \n",
    "    return words_d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_together(texto):\n",
    "    '''\n",
    "    Cleaning text\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    words = cleaning_txt(texto)\n",
    "    words = tokens_alldocs(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchlist(List1, List2):\n",
    "    '''\n",
    "    matching the list of words in each doc and the main_words\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    List3 = []\n",
    "    for i in range(len(List1)):\n",
    "        w = List1[i]\n",
    "        for j in range(len(List2)):\n",
    "            if List1[i] == List2[j]:\n",
    "                List3.append(w)\n",
    "                break             \n",
    "    return List3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortdictkey(diccionario):\n",
    "    '''\n",
    "    Sorted the dictionary\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    dic_items = diccionario.items()\n",
    "    inv_dic = sorted(dic_items)\n",
    "    return inv_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettingthedictionary(path):\n",
    "    \n",
    "    '''\n",
    "    Prepare the script to ask the client the path to be anlyzed\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    global diccionario\n",
    "    global main_words\n",
    "    global main_words_bydoc\n",
    "    global txt\n",
    "    global dinv_idx\n",
    "\n",
    "    diccionario = {}\n",
    "    main_words = []\n",
    "    txt = []\n",
    "    textobydoc = []\n",
    "       \n",
    "    all_docs = list_docs(path)\n",
    "    #print(\"\\n\")\n",
    "    #print(all_docs)\n",
    "    \n",
    "    for f in all_docs:\n",
    "        #print(f)\n",
    "        #print(files_path+'/'+f)\n",
    "        document = open(path+'/'+f, 'r', encoding='cp1252')\n",
    "        txt.append(document.read().replace(r'\\r', '').replace(r'\\n', ''))\n",
    "        #print(\"\\n\")\n",
    "        #print('============ text in the file. =====================')\n",
    "        #print(txt)\n",
    "\n",
    "\n",
    "    ## getting the main words for all the documents in the path\n",
    "    main_words = all_together(txt)\n",
    "    #print(\"\\n\")\n",
    "    #print(main_words)\n",
    "    \n",
    "    ## getting a dictionary with enumerted values\n",
    "    wdict = {key: idx for idx, key in enumerate(main_words)}\n",
    "    #print(\"\\n\")\n",
    "    #print(wdict) \n",
    "\n",
    "    \n",
    "    ## creating the dictionary where the key is the word idex and the value the documents where is located\n",
    "    for d in all_docs:\n",
    "        document_r = open(path+'/'+d, 'r', encoding='cp1252')\n",
    "        textobydoc.append(document_r.read().replace(r'\\r', '').replace(r'\\n', ''))\n",
    "        words_bydoc = all_together(textobydoc)\n",
    "        main_words_bydoc = matchlist(main_words, words_bydoc)\n",
    "        #print(\"\\n\")\n",
    "        #print(main_words_bydoc)\n",
    "        \n",
    "        main_words_bydoc.sort()\n",
    "        #print(\"\\n\")\n",
    "        #print(main_words_bydoc)\n",
    "         \n",
    "        for w in main_words_bydoc:\n",
    "            if w not in diccionario.keys():\n",
    "                diccionario[w] = [d]\n",
    "            else:\n",
    "                diccionario[w] += [d]\n",
    "                \n",
    "        \n",
    "    #sorting the dictionary by key:word    \n",
    "    inv_dict = sortdictkey(diccionario)\n",
    "        \n",
    "    \n",
    "    #remplacing the key:word by key:index in a new dictionary\n",
    "    dinv_idx = defaultdict(list)   \n",
    "    for k1, v1 in wdict.items():\n",
    "        for k2, v2 in diccionario.items():\n",
    "            if k1 == k2:\n",
    "                dinv_idx[v1].append(v2)\n",
    "\n",
    "    #print(\"\\n\")\n",
    "    #print(diccionario)          \n",
    "    \n",
    "    return inv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querydocs(scanwrd, diccw):\n",
    "    '''\n",
    "    Quering the words provide by the client\n",
    "    \n",
    "    '''\n",
    "    results = {}\n",
    "    for word in diccw:\n",
    "        for w in scanwrd:\n",
    "            if w != word[0]:\n",
    "                pass\n",
    "            else:\n",
    "                results[w] = word[1]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salir ():\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the path where are located the files to scan begining by a /: /Users/jackyb/Projects/GitHub/challenges/dataset_test\n",
      "{'barney': ['1', '2', '3', '4', '5'], 'beginning': ['1', '2', '3', '4', '5'], 'bells': ['1', '2', '3', '4', '5'], 'ben': ['1', '2', '3', '4', '5'], 'boots': ['1', '2', '3', '4', '5'], 'candy': ['1', '2', '3', '4', '5'], 'canes': ['1', '2', '3', '4', '5'], 'carol': ['1', '2', '3', '4', '5'], 'christmas': ['1', '2', '3', '4', '5'], 'dad': ['1', '2', '3', '4', '5'], 'dolls': ['1', '2', '3', '4', '5'], 'door': ['1', '2', '3', '4', '5'], 'em': ['1', '2', '3', '4', '5'], 'every': ['1', '2', '3', '4', '5'], 'everywhere': ['1', '2', '3', '4', '5'], 'five': ['1', '2', '3', '4', '5'], 'front': ['1', '2', '3', '4', '5'], 'glistening': ['1', '2', '3', '4', '5'], 'glow': ['1', '2', '3', '4', '5'], 'go': ['1', '2', '3', '4', '5'], 'grand': ['1', '2', '3', '4', '5'], 'hardly': ['1', '2', '3', '4', '5'], 'heart': ['1', '2', '3', '4', '5'], 'holly': ['1', '2', '3', '4', '5'], 'hopalong': ['1', '2', '3', '4', '5'], 'hope': ['1', '2', '3', '4', '5'], 'hotel': ['1', '2', '3', '4', '5'], 'janice': ['1', '2', '3', '4', '5'], 'jen': ['1', '2', '3', '4', '5'], 'kind': ['1', '2', '3', '4', '5'], 'lanes': ['1', '2', '3', '4', '5'], 'like': ['1', '2', '3', '4', '5'], 'look': ['1', '2', '3', '4', '5'], 'lot': ['1', '2', '3', '4', '5'], 'make': ['1', '2', '3', '4', '5'], 'mind': ['1', '2', '3', '4', '5'], 'mom': ['1', '2', '3', '4', '5'], 'one': ['1', '2', '3', '4', '5'], 'pair': ['1', '2', '3', '4', '5'], 'park': ['1', '2', '3', '4', '5'], 'pistol': ['1', '2', '3', '4', '5'], 'prettiest': ['1', '2', '3', '4', '5'], 'right': ['1', '2', '3', '4', '5'], 'ring': ['1', '2', '3', '4', '5'], 'school': ['1', '2', '3', '4', '5'], 'see': ['1', '2', '3', '4', '5'], 'shoots': ['1', '2', '3', '4', '5'], 'sight': ['1', '2', '3', '4', '5'], 'silver': ['1', '2', '3', '4', '5'], 'sing': ['1', '2', '3', '4', '5'], 'snow': ['1', '2', '3', '4', '5'], 'soon': ['1', '2', '3', '4', '5'], 'start': ['1', '2', '3', '4', '5'], 'store': ['1', '2', '3', '4', '5'], 'sturdy': ['1', '2', '3', '4', '5'], 'sure': ['1', '2', '3', '4', '5'], 'take': ['1', '2', '3', '4', '5'], 'talk': ['1', '2', '3', '4', '5'], 'ten': ['1', '2', '3', '4', '5'], 'thing': ['1', '2', '3', '4', '5'], 'toys': ['1', '2', '3', '4', '5'], 'tree': ['1', '2', '3', '4', '5'], 'wait': ['1', '2', '3', '4', '5'], 'walk': ['1', '2', '3', '4', '5'], 'well': ['1', '2', '3', '4', '5'], 'wish': ['1', '2', '3', '4', '5'], 'within': ['1', '2', '3', '4', '5'], 'around': ['2', '3', '4', '5'], 'boughs': ['2', '3', '4', '5'], 'caroling': ['2', '3', '4', '5'], 'couple': ['2', '3', '4', '5'], 'dancin': ['2', '3', '4', '5'], 'deck': ['2', '3', '4', '5'], 'everyone': ['2', '3', '4', '5'], 'fashioned': ['2', '3', '4', '5'], 'feeling': ['2', '3', '4', '5'], 'get': ['2', '3', '4', '5'], 'halls': ['2', '3', '4', '5'], 'happy': ['2', '3', '4', '5'], 'hear': ['2', '3', '4', '5'], 'holiday': ['2', '3', '4', '5'], 'hop': ['2', '3', '4', '5'], 'hung': ['2', '3', '4', '5'], 'jolly': ['2', '3', '4', '5'], 'later': ['2', '3', '4', '5'], 'let': ['2', '3', '4', '5'], 'merrily': ['2', '3', '4', '5'], 'mistletoe': ['2', '3', '4', '5'], 'morerockin': ['2', '3', '4', '5'], 'new': ['2', '3', '4', '5'], 'old': ['2', '3', '4', '5'], 'party': ['2', '3', '4', '5'], 'pie': ['2', '3', '4', '5'], 'pumpkin': ['2', '3', '4', '5'], 'rockin': ['2', '3', '4', '5'], 'sentimental': ['2', '3', '4', '5'], 'singing': ['2', '3', '4', '5'], 'spirit': ['2', '3', '4', '5'], 'stop': ['2', '3', '4', '5'], 'tries': ['2', '3', '4', '5'], 'us': ['2', '3', '4', '5'], 'voices': ['2', '3', '4', '5'], 'way': ['2', '3', '4', '5'], 'air': ['3', '4', '5'], 'ask': ['3', '4', '5'], 'asking': ['3', '4', '5'], 'awake': ['3', '4', '5'], 'baby': ['3', '4', '5'], 'brightly': ['3', '4', '5'], 'bring': ['3', '4', '5'], 'care': ['3', '4', '5'], 'cause': ['3', '4', '5'], 'children': ['3', '4', '5'], 'claus': ['3', '4', '5'], 'click': ['3', '4', '5'], 'come': ['3', '4', '5'], 'could': ['3', '4', '5'], 'day': ['3', '4', '5'], 'even': ['3', '4', '5'], 'ever': ['3', '4', '5'], 'fills': ['3', '4', '5'], 'fireplace': ['3', '4', '5'], 'hang': ['3', '4', '5'], 'holding': ['3', '4', '5'], 'keep': ['3', '4', '5'], 'know': ['3', '4', '5'], 'laughter': ['3', '4', '5'], 'lights': ['3', '4', '5'], 'list': ['3', '4', '5'], 'magic': ['3', '4', '5'], 'much': ['3', '4', '5'], 'na': ['3', '4', '5'], 'need': ['3', '4', '5'], 'nick': ['3', '4', '5'], 'north': ['3', '4', '5'], 'oh': ['3', '4', '5'], 'outside': ['3', '4', '5'], 'please': ['3', '4', '5'], 'pole': ['3', '4', '5'], 'presents': ['3', '4', '5'], 'quickly': ['3', '4', '5'], 'really': ['3', '4', '5'], 'reindeer': ['3', '4', '5'], 'ringing': ['3', '4', '5'], 'saint': ['3', '4', '5'], 'santa': ['3', '4', '5'], 'send': ['3', '4', '5'], 'shining': ['3', '4', '5'], 'sleigh': ['3', '4', '5'], 'sound': ['3', '4', '5'], 'standing': ['3', '4', '5'], 'stay': ['3', '4', '5'], 'stocking': ['3', '4', '5'], 'tight': ['3', '4', '5'], 'tonight': ['3', '4', '5'], 'toy': ['3', '4', '5'], 'true': ['3', '4', '5'], 'underneath': ['3', '4', '5'], 'upon': ['3', '4', '5'], 'waiting': ['3', '4', '5'], 'wan': ['3', '4', '5'], 'want': ['3', '4', '5'], 'wayi': ['3', '4', '5'], 'babyfeliz': ['4', '5'], 'bottom': ['4', '5'], 'felicidad': ['4', '5'], 'feliz': ['4', '5'], 'merry': ['4', '5'], 'navidad': ['4', '5'], 'prospero': ['4', '5'], 'apart': ['5'], 'away': ['5'], 'bitten': ['5'], 'catch': ['5'], 'cover': ['5'], 'crowded': ['5'], 'cry': ['5'], 'distance': ['5'], 'eye': ['5'], 'eyes': ['5'], 'face': ['5'], 'fire': ['5'], 'fool': ['5'], 'found': ['5'], 'friends': ['5'], 'gave': ['5'], 'give': ['5'], 'god': ['5'], 'guess': ['5'], 'hiding': ['5'], 'ice': ['5'], 'kissed': ['5'], 'last': ['5'], 'love': ['5'], 'lover': ['5'], 'man': ['5'], 'maybe': ['5'], 'meant': ['5'], 'never': ['5'], 'next': ['5'], 'note': ['5'], 'real': ['5'], 'recognize': ['5'], 'rely': ['5'], 'room': ['5'], 'save': ['5'], 'saying': ['5'], 'sent': ['5'], 'shoulder': ['5'], 'shy': ['5'], 'someone': ['5'], 'soul': ['5'], 'special': ['5'], 'still': ['5'], 'surprise': ['5'], 'tears': ['5'], 'tell': ['5'], 'thought': ['5'], 'tired': ['5'], 'tore': ['5'], 'twice': ['5'], 'wrapped': ['5'], 'year': ['5']}\n",
      "\n",
      "\n",
      "======================================= Dictionary ======================================\n",
      "\n",
      "\n",
      "The dictionary from the documents scanned is: \n",
      "\n",
      "\n",
      "defaultdict(<class 'list'>, {0: [['3', '4', '5']], 1: [['5']], 2: [['2', '3', '4', '5']], 3: [['3', '4', '5']], 4: [['3', '4', '5']], 5: [['3', '4', '5']], 6: [['5']], 7: [['3', '4', '5']], 8: [['4', '5']], 9: [['1', '2', '3', '4', '5']], 10: [['1', '2', '3', '4', '5']], 11: [['1', '2', '3', '4', '5']], 12: [['1', '2', '3', '4', '5']], 13: [['5']], 14: [['1', '2', '3', '4', '5']], 15: [['4', '5']], 16: [['2', '3', '4', '5']], 17: [['3', '4', '5']], 18: [['3', '4', '5']], 19: [['1', '2', '3', '4', '5']], 20: [['1', '2', '3', '4', '5']], 21: [['3', '4', '5']], 22: [['1', '2', '3', '4', '5']], 23: [['2', '3', '4', '5']], 24: [['5']], 25: [['3', '4', '5']], 26: [['3', '4', '5']], 27: [['1', '2', '3', '4', '5']], 28: [['3', '4', '5']], 29: [['3', '4', '5']], 30: [['3', '4', '5']], 31: [['3', '4', '5']], 32: [['2', '3', '4', '5']], 33: [['5']], 34: [['5']], 35: [['5']], 36: [['1', '2', '3', '4', '5']], 37: [['2', '3', '4', '5']], 38: [['3', '4', '5']], 39: [['2', '3', '4', '5']], 40: [['5']], 41: [['1', '2', '3', '4', '5']], 42: [['1', '2', '3', '4', '5']], 43: [['1', '2', '3', '4', '5']], 44: [['3', '4', '5']], 45: [['3', '4', '5']], 46: [['1', '2', '3', '4', '5']], 47: [['2', '3', '4', '5']], 48: [['1', '2', '3', '4', '5']], 49: [['5']], 50: [['5']], 51: [['5']], 52: [['2', '3', '4', '5']], 53: [['2', '3', '4', '5']], 54: [['4', '5']], 55: [['4', '5']], 56: [['3', '4', '5']], 57: [['5']], 58: [['3', '4', '5']], 59: [['1', '2', '3', '4', '5']], 60: [['5']], 61: [['5']], 62: [['5']], 63: [['1', '2', '3', '4', '5']], 64: [['5']], 65: [['2', '3', '4', '5']], 66: [['5']], 67: [['1', '2', '3', '4', '5']], 68: [['1', '2', '3', '4', '5']], 69: [['1', '2', '3', '4', '5']], 70: [['5']], 71: [['1', '2', '3', '4', '5']], 72: [['5']], 73: [['2', '3', '4', '5']], 74: [['3', '4', '5']], 75: [['2', '3', '4', '5']], 76: [['1', '2', '3', '4', '5']], 77: [['2', '3', '4', '5']], 78: [['1', '2', '3', '4', '5']], 79: [['5']], 80: [['3', '4', '5']], 81: [['2', '3', '4', '5']], 82: [['1', '2', '3', '4', '5']], 83: [['2', '3', '4', '5']], 84: [['1', '2', '3', '4', '5']], 85: [['1', '2', '3', '4', '5']], 86: [['1', '2', '3', '4', '5']], 87: [['2', '3', '4', '5']], 88: [['5']], 89: [['1', '2', '3', '4', '5']], 90: [['1', '2', '3', '4', '5']], 91: [['2', '3', '4', '5']], 92: [['3', '4', '5']], 93: [['1', '2', '3', '4', '5']], 94: [['5']], 95: [['3', '4', '5']], 96: [['1', '2', '3', '4', '5']], 97: [['5']], 98: [['2', '3', '4', '5']], 99: [['3', '4', '5']], 100: [['2', '3', '4', '5']], 101: [['3', '4', '5']], 102: [['1', '2', '3', '4', '5']], 103: [['3', '4', '5']], 104: [['1', '2', '3', '4', '5']], 105: [['1', '2', '3', '4', '5']], 106: [['5']], 107: [['5']], 108: [['3', '4', '5']], 109: [['1', '2', '3', '4', '5']], 110: [['5']], 111: [['5']], 112: [['5']], 113: [['2', '3', '4', '5']], 114: [['4', '5']], 115: [['1', '2', '3', '4', '5']], 116: [['2', '3', '4', '5']], 117: [['1', '2', '3', '4', '5']], 118: [['2', '3', '4', '5']], 119: [['3', '4', '5']], 120: [['3', '4', '5']], 121: [['4', '5']], 122: [['3', '4', '5']], 123: [['5']], 124: [['2', '3', '4', '5']], 125: [['5']], 126: [['3', '4', '5']], 127: [['3', '4', '5']], 128: [['5']], 129: [['3', '4', '5']], 130: [['2', '3', '4', '5']], 131: [['1', '2', '3', '4', '5']], 132: [['3', '4', '5']], 133: [['1', '2', '3', '4', '5']], 134: [['1', '2', '3', '4', '5']], 135: [['2', '3', '4', '5']], 136: [['2', '3', '4', '5']], 137: [['1', '2', '3', '4', '5']], 138: [['3', '4', '5']], 139: [['3', '4', '5']], 140: [['3', '4', '5']], 141: [['1', '2', '3', '4', '5']], 142: [['4', '5']], 143: [['2', '3', '4', '5']], 144: [['3', '4', '5']], 145: [['5']], 146: [['3', '4', '5']], 147: [['5']], 148: [['3', '4', '5']], 149: [['5']], 150: [['1', '2', '3', '4', '5']], 151: [['1', '2', '3', '4', '5']], 152: [['3', '4', '5']], 153: [['2', '3', '4', '5']], 154: [['5']], 155: [['3', '4', '5']], 156: [['3', '4', '5']], 157: [['5']], 158: [['5']], 159: [['1', '2', '3', '4', '5']], 160: [['1', '2', '3', '4', '5']], 161: [['3', '4', '5']], 162: [['5']], 163: [['2', '3', '4', '5']], 164: [['3', '4', '5']], 165: [['1', '2', '3', '4', '5']], 166: [['5']], 167: [['5']], 168: [['1', '2', '3', '4', '5']], 169: [['1', '2', '3', '4', '5']], 170: [['1', '2', '3', '4', '5']], 171: [['2', '3', '4', '5']], 172: [['3', '4', '5']], 173: [['1', '2', '3', '4', '5']], 174: [['5']], 175: [['1', '2', '3', '4', '5']], 176: [['5']], 177: [['3', '4', '5']], 178: [['5']], 179: [['2', '3', '4', '5']], 180: [['3', '4', '5']], 181: [['1', '2', '3', '4', '5']], 182: [['3', '4', '5']], 183: [['5']], 184: [['3', '4', '5']], 185: [['2', '3', '4', '5']], 186: [['1', '2', '3', '4', '5']], 187: [['1', '2', '3', '4', '5']], 188: [['1', '2', '3', '4', '5']], 189: [['5']], 190: [['1', '2', '3', '4', '5']], 191: [['1', '2', '3', '4', '5']], 192: [['5']], 193: [['5']], 194: [['1', '2', '3', '4', '5']], 195: [['1', '2', '3', '4', '5']], 196: [['5']], 197: [['3', '4', '5']], 198: [['5']], 199: [['3', '4', '5']], 200: [['5']], 201: [['3', '4', '5']], 202: [['1', '2', '3', '4', '5']], 203: [['1', '2', '3', '4', '5']], 204: [['2', '3', '4', '5']], 205: [['3', '4', '5']], 206: [['5']], 207: [['3', '4', '5']], 208: [['3', '4', '5']], 209: [['2', '3', '4', '5']], 210: [['2', '3', '4', '5']], 211: [['1', '2', '3', '4', '5']], 212: [['3', '4', '5']], 213: [['1', '2', '3', '4', '5']], 214: [['3', '4', '5']], 215: [['3', '4', '5']], 216: [['2', '3', '4', '5']], 217: [['3', '4', '5']], 218: [['1', '2', '3', '4', '5']], 219: [['1', '2', '3', '4', '5']], 220: [['1', '2', '3', '4', '5']], 221: [['5']], 222: [['5']]})\n",
      "\n",
      "\n",
      "Processing Start Time: 13:56\n",
      "Processing End Time: 13:56\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "\n",
    "    \n",
    "    ##tracking the time at the begining of the process\n",
    "    time_start = time.localtime()\n",
    "    \n",
    "    ## building the inverted index dictionary    \n",
    "    inverted_dictionary = {}\n",
    "    queriew = []\n",
    "    \n",
    "    files_path = gettingpath()\n",
    "    inv_dictionary = gettingthedictionary(files_path)\n",
    "    #print(\"\\n\")\n",
    "    #print(inv_dictionary)\n",
    "    \n",
    "    \n",
    "    #executing the query\n",
    "    print(diccionario)\n",
    "    queryresult = querydocs(queriew, diccionario)\n",
    "    \n",
    "    time_end = time.localtime()\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"======================================= Dictionary ======================================\")\n",
    "    print(\"\\n\")\n",
    "    print('The dictionary from the documents scanned is: ')\n",
    "    print(\"\\n\")\n",
    "    print( dinv_idx)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print ('Processing Start Time: %.2d:%.2d' % (time_start.tm_hour, time_start.tm_min))\n",
    "    print ('Processing End Time: %.2d:%.2d' % (time_end.tm_hour, time_end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
