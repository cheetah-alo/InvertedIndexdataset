{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#E8846F\">Dictionary Inverted Index <a name=\"id6\"></a>\n",
    "   Position: Junior Data Eng at Glofox \n",
    "   Asigment: Build a inverted index  \n",
    "   Candidate: Jacky Barraza  \n",
    "\n",
    "This notebook is a key part of the requirement done by Sam to Jon Snow. The main goal of the whole project is to build a friendly solution to search for information in documents from keywords. \n",
    "    \n",
    "Here is presented a program to create the inverted index of documents. Because Sam commneted to Jon about the huge collection in the Castle Black's, Jon Snow design a system to transfer the documents to hdfs, which will allow handling big data through the distributed file system and speed up the search. \n",
    "    \n",
    "The result of this process is saved in a document in a directory output directory in hdfs. \n",
    "    \n",
    "As part of getting a better job, Jon Snow has some ideas for improving the process. This is not developed in this notebook but is mentioned for tracking improvements ideas.\n",
    "    \n",
    "*The code can be implemented also doing streaming using Kafka or Nifi. Once the connection is done in a notebook running spark streaming, the process will get the new information added to the path in hdfs (new main repository of documents). The new data will be analyzed and **the dictionary of the inverted index** will be updated. The output is proposed to  be saved in a database such as MongoDB, in a bucket or where the data lake is located.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import collect_list\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .appName('inverted_dictionary')\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFilePath(path):\n",
    "    '''\n",
    "    Extract the file path\n",
    "    '''\n",
    "    base = os.path.basename(path)\n",
    "    return os.path.splitext(base)[0]\n",
    "\n",
    "#taking only the name of files\n",
    "\n",
    "udfFileName = func.udf(lambda f: extractFilePath(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|               value|book|\n",
      "+--------------------+----+\n",
      "|Project Gutenberg...|   0|\n",
      "|This is our 3rd e...|   0|\n",
      "|                    |   0|\n",
      "|                    |   0|\n",
      "|Copyright laws ar...|   0|\n",
      "|the copyright law...|   0|\n",
      "|                    |   0|\n",
      "|Please take a loo...|   0|\n",
      "|We encourage you ...|   0|\n",
      "|electronic path o...|   0|\n",
      "|                    |   0|\n",
      "|                    |   0|\n",
      "|**Welcome To The ...|   0|\n",
      "|                    |   0|\n",
      "|**Etexts Readable...|   0|\n",
      "|                    |   0|\n",
      "|*These Etexts Pre...|   0|\n",
      "|                    |   0|\n",
      "|Information on co...|   0|\n",
      "|further informati...|   0|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading the path with the files and taking only the name of the files for the df\n",
    "book_lines = spark.read.text('hdfs://localhost:9000/Users/jackyb/PycharmProjects/4_Inverted_Index_dataset/data/input/dataset_test')\\\n",
    "    .withColumn('book', udfFileName(func.input_file_name()))\n",
    "book_lines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|book|       word|\n",
      "+----+-----------+\n",
      "|   0|    project|\n",
      "|   0|  gutenberg|\n",
      "|   0|      etext|\n",
      "|   0|shakespeare|\n",
      "|   0|      first|\n",
      "|   0|      folio|\n",
      "|   0|      plays|\n",
      "|   0|       this|\n",
      "|   0|        our|\n",
      "|   0|    edition|\n",
      "+----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating a dataframe made of words and files name. \n",
    "#For this, we expands the array output from the split function with the func explode\n",
    "#fun explode made a similar process to flatmap from the RDD\n",
    "#func lenght is taking words > to 2 characters to avoid some of the stopwords\n",
    "\n",
    "book_words = book_lines\\\n",
    "    .select('book', func.explode(func.split('value', '[^a-zA-Z]+')).alias('word'))\\\n",
    "    .where((func.length('word') > 2))\\\n",
    "    .select('book', func.trim(func.lower(func.col('word'))).alias('word'))\n",
    "\n",
    "book_words.show(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a df which agroup by word and create a list of all the doc where the word was found and sorting it. \n",
    "\n",
    "dict = book_words.distinct().groupby('word')\\\n",
    "       .agg(func.sort_array(collect_list('book')).alias('book'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|          word|                book|\n",
      "+--------------+--------------------+\n",
      "|        abazai|                 [2]|\n",
      "|    abruptness|     [11, 15, 43, 6]|\n",
      "|  accumulation|[0, 1, 15, 18, 2,...|\n",
      "|       acheron|              [0, 3]|\n",
      "|       acidity|                 [6]|\n",
      "|      affixing|                [18]|\n",
      "|alimentiveness|                [31]|\n",
      "|     ammonites|            [23, 34]|\n",
      "|      antennae|        [18, 28, 34]|\n",
      "|     antiphony|                [41]|\n",
      "|     apathaton|                 [0]|\n",
      "| apprehensions|[0, 1, 13, 15, 26...|\n",
      "|arctopithecine|                [18]|\n",
      "|       argueil|                [26]|\n",
      "|     arguments|[0, 1, 10, 11, 12...|\n",
      "+--------------+--------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a index column, so as we have unique words, the index will correspond a that word ordered alphabetic desc.\n",
    "\n",
    "w = Window().orderBy(\"word\")\n",
    "\n",
    "df = dict.select(row_number().over(w).alias(\"word_idx\"), col(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+\n",
      "|word_idx|       word|                book|\n",
      "+--------+-----------+--------------------+\n",
      "|       1|     aachen|                [21]|\n",
      "|       2|        aah|            [14, 41]|\n",
      "|       3|     aahmes|                 [8]|\n",
      "|       4|   aanaware|                [27]|\n",
      "|       5|     aaraaf|            [30, 34]|\n",
      "|       6|   aarenias|                [27]|\n",
      "|       7|      aaron|[0, 1, 12, 19, 20...|\n",
      "|       8|     aarons|                 [0]|\n",
      "|       9|        aba|                 [9]|\n",
      "|      10|      aback|[11, 12, 13, 15, ...|\n",
      "|      11|     abacus|                [23]|\n",
      "|      12|    abaddon|                 [9]|\n",
      "|      13|      abaft|             [4, 43]|\n",
      "|      14|      abana|             [2, 44]|\n",
      "|      15|    abandon|[0, 1, 11, 12, 15...|\n",
      "|      16|  abandoned|[0, 1, 11, 12, 15...|\n",
      "|      17| abandoning|[1, 10, 11, 17, 2...|\n",
      "|      18|abandonment|[12, 2, 21, 22, 2...|\n",
      "|      19|   abandons|                 [1]|\n",
      "|      20|     abaout|                [14]|\n",
      "+--------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|word_idx|                book|\n",
      "+--------+--------------------+\n",
      "|       1|                [21]|\n",
      "|       2|            [14, 41]|\n",
      "|       3|                 [8]|\n",
      "|       4|                [27]|\n",
      "|       5|            [30, 34]|\n",
      "|       6|                [27]|\n",
      "|       7|[0, 1, 12, 19, 20...|\n",
      "|       8|                 [0]|\n",
      "|       9|                 [9]|\n",
      "|      10|[11, 12, 13, 15, ...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#selecting only the filds need in another df\n",
    "inv_dictionary = df.select('word_idx', 'book').show(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
